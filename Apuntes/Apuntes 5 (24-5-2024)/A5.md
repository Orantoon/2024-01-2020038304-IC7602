### **Redes (IC7602)** – Semestre 1, 2024
### **Apuntes 5** – Clase 24-5-2024
### David Suárez Acosta – 2020038304
____

La web es un conjunto de páginas que tienen vínculos entre ellas. El internet es una colección de páginas que tienen *hipervínculos*. estas páginas no necesariamente pertenecen al mismo host.

Antes existían *robots* o también llamados "spiders" o "web cralers" que usaba Google y otras marcas. Cuando se configuran estos robots, estos se conectan a una página web y hacen un text extract (eso quiere decir que agarra el HTML que importa algo de información) y luego se hace una extracción de los hipervínculos. Cuando se hace un crawling, se definen parámetros para ver cual va a ser el "deep" del crawling. También debe haber un control de qué páginas se han revisado para que los robots no se queden pegados en un ciclo.

Los URIs y URLs son estándares que permiten establecer direcciónes en internet y completamente local. Los URLs son un subgrupo de los URIs. Las cookies permiten guardar los datos de inicio de sesión.

Cuando se tiene una aplicación *stateless* no se tiene un estado que tenga que permanecer en CPU y en memoria. Cuando se habla de *statefull* se habla de una aplicación que tiene que persistir datos en disco (Base de Datos) y en memoria.

Los métodos de HTTP principales son:

- **GET**: leer una página web.
- **POST**: adjuntar a una página web.
- **PUT**: almacenar una página web.
- **DELETE**: eliminar una página web.

